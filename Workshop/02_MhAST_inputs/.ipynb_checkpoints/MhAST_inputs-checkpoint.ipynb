{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4b97e3-b7ac-4ced-b5e7-84cb56ef91ad",
   "metadata": {},
   "source": [
    "## This notebook is for preparation of MhAST joint dataset (WLcondQ and QcondWL). We use conditional sampling method (<a href = \"https://iopscience.iop.org/article/10.1088/1748-9326/aad400\">Ward et al., (2018)</a>, <a href = \"https://nhess.copernicus.org/articles/20/489/2020/\">Couasnon et al., (2020)</a>) in which two joint dataset each based on annual maxima of one variable and maximum value for the other variable within its $\\pm$1 day interval is created. Throughout our project, we name these two datasets as WLcondQ (maximum water level conditioned by annual maxima of river flow) and QcondWL (maximum river flow conditionned by annual maxima water level). We use <a href = \"https://github.com/Bijan55699/compound-flooding/blob/master/WLcondQ_QcondWL.py\">this</a> python script to create these datasets. Here we use the script for creating the joint dataset for Richelieu river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12109559-125b-4023-86b1-2e121ac2b46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mohammad/Dossier_travail/705300_rehaussement_marin/Workshop/MhAST_inputs/Richelieu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# %% Inputs:\n",
    "\n",
    "# 1. Name of the river outlet for which we want to create the joint datasets.\n",
    "# 2. Path to the daily flow data (excel file) provided by DEH and \n",
    "# 3. Path to the hourly water level data (MPO simulations)\n",
    "# 4. Path to write the outputs\n",
    "\n",
    "# Output:\n",
    "\n",
    "# WLcondQ/QcondWL datasets in csv format\n",
    "\n",
    "name = 'Richelieu'\n",
    "serie = 'QcondWL'\n",
    "pth_riverflow_data = '/home/mohammad/Dossier_travail/705300_rehaussement_marin/Workshop/MhAST_inputs/Extraction_Qjourn_Ouranos.xlsx' # This is daily data\n",
    "pth_water_level_data = '/home/mohammad/Dossier_travail/705300_rehaussement_marin/Workshop/MhAST_inputs/wl_data.csv' # This is hourly data\n",
    "pth_output = os.path.join('/home/mohammad/Dossier_travail/705300_rehaussement_marin/Workshop/MhAST_inputs/'+name) # This is daily data\n",
    "#pth_output = os.path.join('/home/mohammad/Dossier_travail/705300_rehaussement_marin/Workshop/MhAST_inputs/'+name+ '/'+serie + '.csv') # This is daily data\n",
    "pth_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb3d5d0-3e08-4ffb-887f-42437156ba16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The main function\n",
    "def CreateJointDataset(name,pth_riverflow_data,pth_water_level_data,pth_output,serie):\n",
    "    # check to see if the directory for output exist, otherwise it creates\n",
    "    isexist = os.path.exists(pth_output)\n",
    "    if not isexist:\n",
    "        os.makedirs(pth_output)\n",
    "        print('a new directory with the name of river outlet is created!')\n",
    "        \n",
    "    data_Q = pd.read_excel(pth_riverflow_data, index_col = None)\n",
    "    st = {'Yamaska':'MONT00003', 'Richelieu':'MONT00502', 'Saint_Jacques':'MONT01296', 'Saint_Regis':'MONT01317', 'Chateauguay':'MONT01335', 'Maskinonge':'SLNO00496', 'Assomption':'SLNO00563', 'du_Loup':'SLNO00847', 'Saint_Maurice':'SLNO00930',\n",
    "      'Batiscan':'SLNO02927', 'Becancour':'SLSO00767', 'Nicolet':'SLSO00941', 'Saint_Francois':'SLSO01193',\n",
    "      'A_Mars':'SAGU00175', 'Sainte_Anne':'SLNO03193', 'Jacques_Cartier':'SLNO00347', 'Etchemin':'SLSO02381',\n",
    "      'Ristigouche':'GASP00913', 'Petit_Cascapedia':'GASP01528', 'York':'GASP02158', 'au_Renard':'GASP00038','Matane':'GASP02848', 'Mitis':'GASP03111', 'Outardes':'CNDA00096', 'Ha_Ha':'SAGU00151', 'Petit_Saguenay':'SAGU00012',\n",
    "      'Gouffre':'SLNO00195', 'RivSud':'SLSO02566', 'Montmorency':'SLNO00294', 'Chaudiere':'SLSO00003','Saint_Charles':'SLNO00004',\n",
    "      'Chicoutimi':'SAGU00288','Moulin':'SAGU00279', 'Saint_Jean':'SAGU00068'}  \n",
    "\n",
    "    idd = st[name]\n",
    "    Q = pd.DataFrame({'Q(m3/s)':data_Q[idd][0:],\n",
    "                       'Date':data_Q['Date'][0:].astype('datetime64[ns]')})\n",
    "    \n",
    "    # Transform the Date to year, month, and day columns\n",
    "    Q['year'] = pd.DatetimeIndex(Q['Date']).year\n",
    "    Q['month'] = pd.DatetimeIndex(Q['Date']).month\n",
    "    Q['day'] = pd.DatetimeIndex(Q['Date']).day\n",
    "    \n",
    "    # Calculating the annual maxima\n",
    "    \n",
    "    Q_annual_max = Q.loc[Q.groupby(\"year\")[\"Q(m3/s)\"].idxmax()]\n",
    "    Q_annual_max.reset_index(inplace=True) \n",
    "   \n",
    "    data_wl= pd.read_csv(pth_water_level_data, index_col = None)\n",
    "    data_wl['Date'] = pd.to_datetime(data_wl['Date'])\n",
    "    data_wl = data_wl.set_index('Date')\n",
    "    data_wl = data_wl.loc['1968-01-01':'2020-12-31']  \n",
    "    \n",
    "    #data_wl = data_wl.loc['1985-01-01':'2020-12-31'] \n",
    "    #data_wl = data_wl.loc['1968-01-01':'1984-12-31']  \n",
    "    \n",
    "    # finding annual maxima along with its day, month, year\n",
    "    \n",
    "    data_wl.reset_index(inplace=True) \n",
    "    data_wl['year'] = pd.DatetimeIndex(data_wl['Date']).year\n",
    "    data_wl['month'] = pd.DatetimeIndex(data_wl['Date']).month\n",
    "    data_wl['day'] = pd.DatetimeIndex(data_wl['Date']).day\n",
    "    \n",
    "    wl_max =data_wl.loc[data_wl.groupby(\"year\")[\"wl(m)\"].idxmax()]\n",
    "    \n",
    "    wl_plt =  data_wl.drop(['year', 'month','day'], axis=1)\n",
    "    wl_plt = wl_plt.set_index('Date')\n",
    "    wl_plt_s = wl_plt.squeeze()\n",
    "    \n",
    "    # Resample the hourly data to daily\n",
    "    data_wl = data_wl.set_index('Date')\n",
    "    wl_daily =data_wl.resample('D')['wl(m)'].agg(['max'])\n",
    "    wl_daily = wl_daily.rename(columns = {'max':'wl(m)'})\n",
    "    \n",
    "    # Dropping the NAN columns \n",
    "    \n",
    "    wl_daily = wl_daily.dropna()\n",
    "    \n",
    "    # adding year, month, day to the daily time series\n",
    "    \n",
    "    wl_daily.reset_index(inplace=True) \n",
    "    wl_daily['year'] = pd.DatetimeIndex(wl_daily['Date']).year\n",
    "    wl_daily['month'] = pd.DatetimeIndex(wl_daily['Date']).month\n",
    "    wl_daily['day'] = pd.DatetimeIndex(wl_daily['Date']).day\n",
    "    \n",
    "    if serie == 'WLcondQ':\n",
    "    \n",
    "        df_WLcondQ = pd.DataFrame({'Date':Q_annual_max['Date'],\n",
    "                           'year':Q_annual_max['year'],\n",
    "                          'month':Q_annual_max['month'],\n",
    "                          'day': Q_annual_max['day'],\n",
    "                          'Qmax':Q_annual_max['Q(m3/s)']})                  \n",
    "        \n",
    "        for index, row in Q_annual_max.iterrows():\n",
    "        #    import pdb; pdb.set_trace()\n",
    "            yQ = row['year']\n",
    "            mQ = row['month']\n",
    "            dQ = row['day']\n",
    "            dQQ = datetime.datetime(yQ,mQ,dQ)\n",
    "            for index2,row in wl_daily.iterrows():\n",
    "                yl = row['year']\n",
    "                ml = row['month']\n",
    "                dl = row['day']\n",
    "                wlc = row['wl(m)']\n",
    "                dll  = datetime.datetime(yl,ml,dl)\n",
    "                if dQQ == dll:\n",
    "                    df_WLcondQ.at[index,'Wlmax_0'] = wlc\n",
    "                elif (dll == dQQ - timedelta(days=1)):\n",
    "                    df_WLcondQ.at[index,'Wlmax_1'] = wlc\n",
    "                elif (dll == dQQ + timedelta(days=1)):\n",
    "                    df_WLcondQ.at[index,'Wlmax+1'] = wlc\n",
    "        \n",
    "        df_WLcondQ = df_WLcondQ.dropna()\n",
    "        \n",
    "        maxValue = df_WLcondQ[['Wlmax_0', 'Wlmax_1','Wlmax+1']].max(axis=1)  #finding maximum value among these three days\n",
    "        df_WLcondQ['Wlmax'] = maxValue\n",
    "        \n",
    "        df_WLcondQ = df_WLcondQ.drop(['Wlmax_0','Wlmax_1','Wlmax+1'], axis=1)\n",
    "        \n",
    "        \n",
    "        # Write results to a csv file\n",
    "        df_WLcondQ.to_csv(os.path.join(pth_output,'WLcondQ.csv'),mode='w',index=False)\n",
    "        \n",
    "        tau, p_value_K = stats.kendalltau(df_WLcondQ['Qmax'], df_WLcondQ['Wlmax'],nan_policy='omit')\n",
    "        rho, p_value_Sp = stats.spearmanr(df_WLcondQ['Qmax'], df_WLcondQ['Wlmax'],nan_policy='omit')\n",
    "        print('WLcondQ joint dataset dependence significance:')\n",
    "        print('Kendalls tau = ', tau, 'P-value', p_value_K )\n",
    "        print('Spearman rho = ', rho, 'P-value', p_value_Sp )\n",
    "    else:\n",
    "        # Constructing QcondWL dataframe\n",
    "    \n",
    "        df_QcondWL = pd.DataFrame({'Date':wl_max['Date'],\n",
    "                           'year':wl_max['year'],\n",
    "                          'month':wl_max['month'],\n",
    "                          'day': wl_max['day'],\n",
    "                          'Wlmax':wl_max['wl(m)']})\n",
    "                          \n",
    "        \n",
    "        for index, row in wl_max.iterrows():\n",
    "            yl = row['year']\n",
    "            ml = row['month']\n",
    "            dl = row['day']\n",
    "            dll = datetime.datetime(yl,ml,dl)\n",
    "            for index2,row in Q.iterrows():\n",
    "                yQ = row['year']\n",
    "                mQ = row['month']\n",
    "                dQ = row['day']\n",
    "                Qd = row['Q(m3/s)']\n",
    "                dQQ = datetime.datetime(yQ,mQ,dQ)\n",
    "                if (dQQ == dll):\n",
    "                    df_QcondWL.at[index,'Qmax_0'] = Qd\n",
    "                elif (dQQ == dll - timedelta(days=1)):\n",
    "                    df_QcondWL.at[index,'Qmax_1'] = Qd\n",
    "                elif (dQQ == dll + timedelta(days=1)):\n",
    "                    df_QcondWL.at[index,'Qmax+1'] = Qd \n",
    "        \n",
    "        maxValue = df_QcondWL[['Qmax_0', 'Qmax_1','Qmax+1']].max(axis=1)  #finding maximum value among these three days\n",
    "        df_QcondWL['Qmax'] = maxValue\n",
    "        \n",
    "        df_QcondWL = df_QcondWL.drop(['Qmax_0', 'Qmax_1','Qmax+1'], axis=1)\n",
    "        \n",
    "        # Write results to a .csv file\n",
    "        df_QcondWL = df_QcondWL.dropna()\n",
    "        df_QcondWL.to_csv(os.path.join(pth_output,'QcondWL.csv'),mode='w',index=False)\n",
    "        \n",
    "        # calculating correlation    \n",
    "        \n",
    "        tau, p_value_K = stats.kendalltau(df_QcondWL['Qmax'], df_QcondWL['Wlmax'],nan_policy='omit')\n",
    "        rho, p_value_Sp = stats.spearmanr(df_QcondWL['Qmax'], df_QcondWL['Wlmax'],nan_policy='omit')\n",
    "        print('QcondWL joint dataset dependence significance:')\n",
    "        print('Kendalls tau = ', tau, 'P-value', p_value_K )\n",
    "        print('Spearman rho = ', rho, 'P-value', p_value_Sp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02beba5d-c954-420d-aab6-b57beab1b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QcondWL joint dataset dependence significance:\n",
      "Kendalls tau =  0.24990578502402783 P-value 0.008996417169133724\n",
      "Spearman rho =  0.36005037534598433 P-value 0.008746185116581767\n"
     ]
    }
   ],
   "source": [
    "CreateJointDataset(name,pth_riverflow_data,pth_water_level_data,pth_output,serie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d126ab1-2a02-4584-9cf5-4d2472c5093c",
   "metadata": {},
   "source": [
    "## These datasets can be used in MhAST for compound flooding analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
